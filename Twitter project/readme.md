**Brief**

In this project, I explored a large dataset of tweets collected from
the Twitter API. There are several data analysis and critical reflection tasks that I completed.
The main aim of the project is to use the data supplied to find real-world events
that happened in Europe during a specified period (i.e. March 2020). As well as working through the
technical challenges of handling a large and complex dataset, I undertook
several data analysis tasks to identify and characterise events.

**Dataset**

The dataset consists of tweets collected from the Twitter API during the
period March 1st to March 31st 2020. These tweets were collected using a
geographical filter specifying a rectangular box over Europe. The bounding box used
is specified by (longitude,latitude) coordinates. The lower-left corner is at (-24.5,
34.8) and the upper-right is at (69.1, 81.9). No keyword or other thematic filters were
applied, so the dataset should contain all tweets that Twitter can identify as
originating from the specified region, irrespective of their topic/content.The data is
available for download as a number of compressed files each covering 1 day of data.
The whole dataset is large (typically around 450Mb per day when compressed) and
contains millions of tweets.
Each file contains a tweet on every line. Tweets are stored as JSON objects, as
described in the Twitter developer documentation. In particular, as these are located
tweets, pay attention to the difference between “place” tags and “coordinates” tags.

**Data Pre-processing**

Out of all the data available in JSON form, 32 separate db tables were created. 31 tables are
maintaining the tweets related data each containing the tweet data for a day of March month.
They have attributes like tweet_id, user_id, tweet_text, created_at, user_mentions, boud_lat,
boundlong etc. These attributes are important enough to do all the required data analysis to
answer the questions. The primary key for these 31 tables is tweet_id which is unique. One table
is maintained for user details like user_id and user_name. Each table has tweet data of a day of
March month that is relevant to complete all the tasks. In addition, a user table is also created to
maintain user data. SQLITE database is used for storing this data.

**Reflection**

Social media has interesting use cases in a variety of research disciplines, such as media, communication, sociology, political science, computer science, engineering, etc. Twitter being one of the most used social media platforms remains an appropriate choice to get data to conduct such research [1].  Twitter data can be helpful in analyzing and management of emergency situations or extreme circumstances. These circumstances can be riots, natural disasters, and other crisis events. Authorities concerned with emergency management can utilize Twitter data to improve their position in responding to emergency situations. Twitter data contain valuable metadata, including geospatial data, such as precise latitude and longitude coordinates of the tweet location. Tweeter API is an open and powerful API, which makes it easy for researchers and developers to use it. Twitter has a strong hashtag culture that makes it easy to center the research on a particular topic.

However, there are also some weaknesses of Twitter data. It is not always true that users will discuss a trendy or interesting topic on Twitter. So, it is not always possible to gauge an event that is in trend from the tweets. Using hashtags or certain keywords to retrieve data related to a particular topic may not always work. Language also poses a challenge when you want to retrieve the data in your topic of interest across multiple geographical locations. 
More problematic could be the bias in Twitter data. For e.g., filtering the tweet data by certain keywords or hashtags may introduce a bias. Also, link-baiting in popular hashtags amounts to spam at a large scale. It is difficult to identify a real or fictitious user. Fictitious or automatic accounts are set up to increase users’ followers and popularity. So, there are a lot of fake accounts and retweets on Twitter[3].This was evident from the word clouds studied in this assignment. For e.g. FoodwasteFree hashtag trends can be verified to be generated by some automated accounts.

There also ethical and legal concerns associated with the Twitter data. It is easy to get the Twitter data through API, but it cannot be said for sure that how many users moderated their behavior accordingly while posting on Twitter as a public space. Therefore, whether Twitter, as an online space, is public or private is a topic of debate. When collecting and using the users’ data, it may not be possible to obtain consent from the users because of the volume of the data. Similarly, it becomes difficult to reproduce tweets in the research publications without having consent from the users.[2]

In conclusion, Twitter as a data source has many benefits, but it also poses many ethical and legal challenges. These challenges could be related to the users’ consent to use their data. There can also be problems of spamming and bias owing to fake and automated accounts. Too much reliance on hashtag words may also be a cause of bias.

References:

[1] Ngai, E.W., Tao, S.S. and Moon, K.K., 2015. Social media research: Theories, constructs, and conceptual frameworks. International journal of information management, 35(1), pp.33-44. https://doi.org/10.1016/j.ijinfomgt.2014.09.004

[2] Moeen Uddin, M., Imran, M. and Sajjad, H., 2014. Understanding Types of Users on Twitter. arXiv, pp.arXiv-1406.

[3] Castillo, C., Mendoza, M. and Poblete, B., 2011, March. Information credibility on twitter. In Proceedings of the 20th international conference on World wide web (pp. 675-684). https://www.researchgate.net/publication/221023878_Information_credibility_on_Twitter
